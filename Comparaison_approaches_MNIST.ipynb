{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381deb5b",
   "metadata": {},
   "source": [
    "# Experiments master thesis ETH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ae4bf",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cba421-4d35-4597-8eb1-1c509da0aa28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('/home/infres/slabbi/Master_thesis/lib/python3.8/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916802cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0ee945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backpack-for-pytorch in ./lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: torchvision<1.0.0,>=0.7.0 in ./lib/python3.8/site-packages (from backpack-for-pytorch) (0.15.1)\n",
      "Requirement already satisfied: einops<1.0.0,>=0.3.0 in ./lib/python3.8/site-packages (from backpack-for-pytorch) (0.6.1)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.9.0 in ./lib/python3.8/site-packages (from backpack-for-pytorch) (1.13.1)\n",
      "Requirement already satisfied: requests in ./lib/python3.8/site-packages (from torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./lib/python3.8/site-packages (from torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (9.5.0)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (1.24.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch<2.0.0,>=1.9.0->backpack-for-pytorch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.8/site-packages (from torch<2.0.0,>=1.9.0->backpack-for-pytorch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch<2.0.0,>=1.9.0->backpack-for-pytorch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch<2.0.0,>=1.9.0->backpack-for-pytorch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch<2.0.0,>=1.9.0->backpack-for-pytorch) (11.7.99)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lib/python3.8/site-packages (from requests->torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests->torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests->torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests->torchvision<1.0.0,>=0.7.0->backpack-for-pytorch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch<2.0.0,>=1.9.0->backpack-for-pytorch) (44.0.0)\n",
      "Requirement already satisfied: wheel in ./lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch<2.0.0,>=1.9.0->backpack-for-pytorch) (0.34.2)\n",
      "Requirement already satisfied: asdfghjkl in ./lib/python3.8/site-packages (0.1a2)\n",
      "Requirement already satisfied: torch>=1.7.0 in ./lib/python3.8/site-packages (from asdfghjkl) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch>=1.7.0->asdfghjkl) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch>=1.7.0->asdfghjkl) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch>=1.7.0->asdfghjkl) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.8/site-packages (from torch>=1.7.0->asdfghjkl) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch>=1.7.0->asdfghjkl) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in ./lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch>=1.7.0->asdfghjkl) (0.34.2)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch>=1.7.0->asdfghjkl) (44.0.0)\n",
      "Requirement already satisfied: laplace-torch in ./lib/python3.8/site-packages (0.1a2)\n",
      "Requirement already satisfied: backpack-for-pytorch in ./lib/python3.8/site-packages (from laplace-torch) (1.5.2)\n",
      "Requirement already satisfied: asdfghjkl in ./lib/python3.8/site-packages (from laplace-torch) (0.1a2)\n",
      "Requirement already satisfied: torchaudio in ./lib/python3.8/site-packages (from laplace-torch) (2.0.1)\n",
      "Requirement already satisfied: torch in ./lib/python3.8/site-packages (from laplace-torch) (1.13.1)\n",
      "Requirement already satisfied: torchvision in ./lib/python3.8/site-packages (from laplace-torch) (0.15.1)\n",
      "Requirement already satisfied: einops<1.0.0,>=0.3.0 in ./lib/python3.8/site-packages (from backpack-for-pytorch->laplace-torch) (0.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.8/site-packages (from torch->laplace-torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.7.99)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (1.24.3)\n",
      "Requirement already satisfied: requests in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (9.5.0)\n",
      "Requirement already satisfied: wheel in ./lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch->laplace-torch) (0.34.2)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch->laplace-torch) (44.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (1.26.15)\n",
      "Requirement already satisfied: laplace in ./lib/python3.8/site-packages (0.1)\n",
      "Requirement already satisfied: laplace-torch in ./lib/python3.8/site-packages (0.1a2)\n",
      "Requirement already satisfied: torchvision in ./lib/python3.8/site-packages (from laplace-torch) (0.15.1)\n",
      "Requirement already satisfied: torchaudio in ./lib/python3.8/site-packages (from laplace-torch) (2.0.1)\n",
      "Requirement already satisfied: torch in ./lib/python3.8/site-packages (from laplace-torch) (1.13.1)\n",
      "Requirement already satisfied: backpack-for-pytorch in ./lib/python3.8/site-packages (from laplace-torch) (1.5.2)\n",
      "Requirement already satisfied: asdfghjkl in ./lib/python3.8/site-packages (from laplace-torch) (0.1a2)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (9.5.0)\n",
      "Requirement already satisfied: requests in ./lib/python3.8/site-packages (from torchvision->laplace-torch) (2.29.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in ./lib/python3.8/site-packages (from torch->laplace-torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in ./lib/python3.8/site-packages (from torch->laplace-torch) (11.10.3.66)\n",
      "Requirement already satisfied: einops<1.0.0,>=0.3.0 in ./lib/python3.8/site-packages (from backpack-for-pytorch->laplace-torch) (0.6.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (1.26.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests->torchvision->laplace-torch) (2022.12.7)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch->laplace-torch) (44.0.0)\n",
      "Requirement already satisfied: wheel in ./lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch->laplace-torch) (0.34.2)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.8/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install backpack-for-pytorch\n",
    "!pip3 install asdfghjkl\n",
    "!pip3 install laplace-torch\n",
    "!pip3 install laplace\n",
    "!python3 -m pip install laplace-torch\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a7952",
   "metadata": {},
   "source": [
    "## Marginal likehood optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9156671f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/slabbi/Master_thesis/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/infres/slabbi/Master_thesis/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/infres/slabbi/Master_thesis/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.convert_parameters import vector_to_parameters\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as td\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from laplace.baselaplace import KronLaplace\n",
    "from laplace.curvature import AsdlGGN\n",
    "\n",
    "\n",
    "GB_FACTOR = 1024 ** 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def expand_prior_precision(prior_prec, model):\n",
    "    theta = parameters_to_vector(model.parameters())\n",
    "    device, P = theta.device, len(theta)\n",
    "    assert prior_prec.ndim == 1\n",
    "    if len(prior_prec) == 1:  # scalar\n",
    "        return torch.ones(P, device=device) * prior_prec\n",
    "    elif len(prior_prec) == P:  # full diagonal\n",
    "        return prior_prec.to(device)\n",
    "    else:\n",
    "        return torch.cat([delta * torch.ones_like(m).flatten() for delta, m\n",
    "                          in zip(prior_prec, model.parameters())])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device):\n",
    "    log_prior_prec_init = np.log(prior_prec_init)\n",
    "    if prior_structure == 'scalar':\n",
    "        log_prior_prec = log_prior_prec_init * torch.ones(1, device=device)\n",
    "    elif prior_structure == 'layerwise':\n",
    "        log_prior_prec = log_prior_prec_init * torch.ones(H, device=device)\n",
    "    elif prior_structure == 'diagonal':\n",
    "        log_prior_prec = log_prior_prec_init * torch.ones(P, device=device)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid prior structure {prior_structure}')\n",
    "    log_prior_prec.requires_grad = True\n",
    "    return log_prior_prec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def valid_performance(model, test_loader, likelihood, criterion, device):\n",
    "    N = len(test_loader.dataset)\n",
    "    perf = 0\n",
    "    nll = 0\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.detach().to(device), y.detach().to(device)\n",
    "        with torch.no_grad():\n",
    "            f = model(X)\n",
    "        if likelihood == 'classification':\n",
    "            perf += (torch.argmax(f, dim=-1) == y).sum() / N\n",
    "        elif likelihood == 'heteroscedastic_regression':\n",
    "            perf += (y.squeeze() + 0.5 * f[:, 0] / f[:, 1]).square().sum() / N\n",
    "        else:\n",
    "            perf += (f - y).square().sum() / N\n",
    "        nll += criterion(f, y) / len(test_loader)\n",
    "    return perf.item(), nll.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min):\n",
    "    n_steps = n_epochs * len(train_loader)\n",
    "    if scheduler == 'exp':\n",
    "        min_lr_factor = lr_min / lr\n",
    "        gamma = np.exp(np.log(min_lr_factor) / n_steps)\n",
    "        return ExponentialLR(optimizer, gamma=gamma)\n",
    "    elif scheduler == 'cos':\n",
    "        return CosineAnnealingLR(optimizer, n_steps, eta_min=lr_min)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid scheduler {scheduler}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model_optimizer(optimizer, model, lr, weight_decay=0):\n",
    "    if optimizer == 'adam':\n",
    "        return Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer == 'sgd':\n",
    "        # fixup parameters should have 10x smaller learning rate\n",
    "        is_fixup = lambda param: param.size() == torch.Size([1])  # scalars\n",
    "        fixup_params = [p for p in model.parameters() if is_fixup(p)]\n",
    "        standard_params = [p for p in model.parameters() if not is_fixup(p)]\n",
    "        params = [{'params': standard_params}, {'params': fixup_params, 'lr': lr / 10.}]\n",
    "        return SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid optimizer {optimizer}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_to_vector(parameters):\n",
    "    return parameters_to_vector([e.grad for e in parameters])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vector_to_gradient(vec, parameters):\n",
    "    return vector_to_parameters(vec, [e.grad for e in parameters])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def marglik_optimization(model,\n",
    "                         train_loader,\n",
    "                         valid_loader=None,\n",
    "                         likelihood='classification',\n",
    "                         prior_structure='layerwise',\n",
    "                         prior_prec_init=1.,\n",
    "                         sigma_noise_init=1.,\n",
    "                         temperature=1.,\n",
    "                         n_epochs=50,\n",
    "                         lr=1e-3,\n",
    "                         lr_min=None,\n",
    "                         optimizer='Adam',\n",
    "                         scheduler='cos',\n",
    "                         n_epochs_burnin=0,\n",
    "                         n_hypersteps=100,\n",
    "                         marglik_frequency=1,\n",
    "                         lr_hyp=1e-1,\n",
    "                         lr_hyp_min=1e-1,\n",
    "                         laplace=KronLaplace,\n",
    "                         backend=AsdlGGN,\n",
    "                         early_stopping=False):\n",
    "    \"\"\"Runs marglik optimization training for a given model and training dataloader.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        torch model\n",
    "    train_loader : DataLoader\n",
    "        pytorch training dataset loader\n",
    "    valid_loader : DataLoader\n",
    "    likelihood : str\n",
    "        'classification', 'regression', 'heteroscedastic_regression'\n",
    "    prior_structure : str\n",
    "        'scalar', 'layerwise', 'diagonal'\n",
    "    prior_prec_init : float\n",
    "        initial prior precision\n",
    "    sigma_noise_init : float\n",
    "        initial observation noise (for regression only)\n",
    "    temperature : float\n",
    "        factor for the likelihood for 'overcounting' data.\n",
    "        Often required when using data augmentation.\n",
    "    n_epochs : int\n",
    "    lr : float\n",
    "        learning rate for model optimizer\n",
    "    lr_min : float\n",
    "        minimum learning rate, defaults to lr and hence no decay\n",
    "        to have the learning rate decay from 1e-3 to 1e-6, set\n",
    "        lr=1e-3 and lr_min=1e-6.\n",
    "    optimizer : str\n",
    "        either 'adam' or 'sgd'\n",
    "    scheduler : str\n",
    "        either 'exp' for exponential and 'cos' for cosine decay towards lr_min\n",
    "    n_epochs_burnin : int default=0\n",
    "        how many epochs to train without estimating and differentiating marglik\n",
    "    n_hypersteps : int\n",
    "        how many steps to take on the hyperparameters when marglik is estimated\n",
    "    marglik_frequency : int\n",
    "        how often to estimate (and differentiate) the marginal likelihood\n",
    "    lr_hyp : float\n",
    "        learning rate for hyperparameters (should be between 1e-3 and 1)\n",
    "    laplace : Laplace\n",
    "        type of Laplace approximation (Kron/Diag/Full)\n",
    "    backend : Backend\n",
    "        AsdlGGN/AsdlEF or BackPackGGN/BackPackEF\n",
    "    stochastic_grad : bool\n",
    "    independent : bool\n",
    "        whether to use independent functional laplace\n",
    "    single_output : bool\n",
    "        whether to use single random output for functional laplace\n",
    "    kron_jac : bool\n",
    "        whether to use kron_jac in the backend\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lap : Laplace\n",
    "        lapalce approximation\n",
    "    model : torch.nn.Module\n",
    "    margliks : list\n",
    "    losses : list\n",
    "    \"\"\"\n",
    "    if lr_min is None:  # don't decay lr\n",
    "        lr_min = lr\n",
    "    device = parameters_to_vector(model.parameters()).device\n",
    "    N = len(train_loader.dataset)\n",
    "    H = len(list(model.parameters()))\n",
    "    P = len(parameters_to_vector(model.parameters()))\n",
    "    best_model_dict = None\n",
    "\n",
    "\n",
    "    # differentiable hyperparameters\n",
    "    hyperparameters = list()\n",
    "    # prior precision\n",
    "    log_prior_prec = get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device)\n",
    "    hyperparameters.append(log_prior_prec)\n",
    "\n",
    "\n",
    "    # set up loss (and observation noise hyperparam)\n",
    "    if likelihood == 'classification':\n",
    "        criterion = CrossEntropyLoss(reduction='mean')\n",
    "        sigma_noise = 1\n",
    "    elif likelihood == 'regression':\n",
    "        criterion = MSELoss(reduction='mean')\n",
    "        log_sigma_noise_init = np.log(sigma_noise_init)\n",
    "        log_sigma_noise = log_sigma_noise_init * torch.ones(1, device=device)\n",
    "        log_sigma_noise.requires_grad = True\n",
    "        hyperparameters.append(log_sigma_noise)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "    # set up model optimizer and scheduler\n",
    "    optimizer = get_model_optimizer(optimizer, model, lr)\n",
    "    scheduler = get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min)\n",
    "\n",
    "\n",
    "    n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * n_hypersteps\n",
    "    hyper_optimizer = Adam(hyperparameters, lr=lr_hyp)\n",
    "    hyper_scheduler = CosineAnnealingLR(hyper_optimizer, n_steps, eta_min=lr_hyp_min)\n",
    "\n",
    "\n",
    "    losses = list()\n",
    "    valid_perfs = list()\n",
    "    valid_nlls = list()\n",
    "    margliks = list()\n",
    "    best_marglik = np.inf\n",
    "\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        epoch_perf = 0\n",
    "        epoch_nll = 0\n",
    "        epoch_log = dict(epoch=epoch)\n",
    "\n",
    "\n",
    "        # standard NN training per batch\n",
    "        torch.cuda.empty_cache()\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.detach().to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if likelihood == 'regression':\n",
    "                sigma_noise = torch.exp(log_sigma_noise).detach()\n",
    "                crit_factor = 1 / temperature / (2 * sigma_noise.square())\n",
    "            else:\n",
    "                crit_factor = 1 / temperature\n",
    "            prior_prec = torch.exp(log_prior_prec).detach()\n",
    "            delta = expand_prior_precision(prior_prec, model)\n",
    "\n",
    "\n",
    "            f = model(X)\n",
    "\n",
    "\n",
    "            theta = parameters_to_vector(model.parameters())\n",
    "            loss = criterion(f, y) + (0.5 * (delta * theta) @ theta) / N / crit_factor\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss += loss.cpu().item() / len(train_loader)\n",
    "            epoch_nll += criterion(f.detach(), y).item() / len(train_loader)\n",
    "            if likelihood == 'regression':\n",
    "                epoch_perf += (f.detach() - y).square().sum() / N\n",
    "            elif likelihood == 'heteroscedastic_regression':\n",
    "                epoch_perf += (y.squeeze() + 0.5 * f[:, 0] / f[:, 1]).square().sum() / N\n",
    "            else:\n",
    "                epoch_perf += torch.sum(torch.argmax(f.detach(), dim=-1) == y).item() / N\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "        logging.info(f'MARGLIK[epoch={epoch}]: train. perf={epoch_perf:.2f}; loss={epoch_loss:.5f}; nll={epoch_nll:.5f}')\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        llr = scheduler.get_last_lr()[0]\n",
    "        epoch_log.update({'train/loss': epoch_loss, 'train/nll': epoch_nll, 'train/perf': epoch_perf, 'train/lr': llr})\n",
    "        # compute validation error to report during training\n",
    "        if valid_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                if likelihood == 'regression':\n",
    "                    def val_criterion(f, y):\n",
    "                        assert f.shape == y.shape\n",
    "                        log_lik = Normal(loc=f, scale=sigma_noise).log_prob(y)\n",
    "                        return -log_lik.mean()\n",
    "                else:\n",
    "                    val_criterion = criterion\n",
    "                val_perf, val_nll = valid_performance(model, valid_loader, likelihood, val_criterion, device)\n",
    "                valid_perfs.append(val_perf)\n",
    "                valid_nlls.append(val_nll)\n",
    "                logging.info(f'MARGLIK[epoch={epoch}]: valid. perf={val_perf:.2f}; nll={val_nll:.5f}.')\n",
    "                epoch_log.update({'valid/perf': val_perf, 'valid/nll': val_nll})\n",
    "\n",
    "\n",
    "        # only update hyperparameters every \"Frequency\" steps after \"burnin\"\n",
    "        if (epoch % marglik_frequency) != 0 or epoch < n_epochs_burnin:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 1. fit laplace approximation\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        sigma_noise = 1 if likelihood != 'regression' else torch.exp(log_sigma_noise)\n",
    "        prior_prec = torch.exp(log_prior_prec)\n",
    "        lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,\n",
    "                        temperature=temperature, backend=backend)\n",
    "        lap.fit(train_loader)\n",
    "        # first optimize prior precision jointly with direct marglik grad\n",
    "        margliks_local = list()\n",
    "        for i in range(n_hypersteps):\n",
    "            hyper_optimizer.zero_grad()\n",
    "            sigma_noise = None if likelihood != 'regression' else torch.exp(log_sigma_noise)\n",
    "            prior_prec = torch.exp(log_prior_prec)\n",
    "            marglik = -lap.log_marginal_likelihood(prior_prec, sigma_noise) / N\n",
    "            marglik.backward()\n",
    "            margliks_local.append(marglik.item())\n",
    "            hyper_optimizer.step()\n",
    "            hyper_scheduler.step()\n",
    "\n",
    "\n",
    "        marglik = margliks_local[-1]\n",
    "\n",
    "\n",
    "        if likelihood == 'regression':\n",
    "            epoch_log['hyperparams/sigma_noise'] = torch.exp(log_sigma_noise.detach()).cpu().item()\n",
    "        epoch_log['train/marglik'] = marglik\n",
    "        margliks.append(marglik)\n",
    "        del lap\n",
    "\n",
    "\n",
    "        # early stopping on marginal likelihood\n",
    "        if early_stopping and (margliks[-1] < best_marglik):\n",
    "            best_model_dict = deepcopy(model.state_dict())\n",
    "            best_precision = deepcopy(prior_prec.detach())\n",
    "            best_sigma = 1 if likelihood != 'regression' else deepcopy(sigma_noise.detach())\n",
    "            best_marglik = margliks[-1]\n",
    "\n",
    "\n",
    "    if early_stopping and (best_model_dict is not None):\n",
    "        model.load_state_dict(best_model_dict)\n",
    "        sigma_noise = best_sigma\n",
    "        prior_prec = best_precision\n",
    "    else:\n",
    "        sigma_noise = 1 if sigma_noise is None else sigma_noise\n",
    "\n",
    "\n",
    "    lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,\n",
    "                  temperature=temperature, backend=backend)\n",
    "    lap.fit(train_loader)\n",
    "    return lap, model, margliks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cf001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from PBB.pbb.models import ProbNNet4l, NNet4l, CNNet4l\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "BATCH_SIZE = 250\n",
    "TRAIN_EPOCHS = 1\n",
    "DELTA = 0.025\n",
    "DELTA_TEST = 0.01\n",
    "PRIOR = 'rand'\n",
    "dropout_prob = 0.0\n",
    "SIGMAPRIOR = 0.03\n",
    "rho_prior = math.log(math.exp(SIGMAPRIOR)-1.0)\n",
    "PMIN = 1e-5\n",
    "KL_PENALTY = 0.1\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.95\n",
    "LEARNING_RATE_PRIOR = 0.005\n",
    "MOMENTUM_PRIOR = 0.99\n",
    "\n",
    "net0 = NNet4l(dropout_prob=dropout_prob, device=device).to(device)\n",
    "\n",
    "net = CNNet4l(dropout_prob=dropout_prob).to(device)\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "lap, model, margliks = marglik_optimization(net,\n",
    "                                             train_loader,\n",
    "                                             valid_loader=test_loader,\n",
    "                                             likelihood='classification',\n",
    "                                             prior_structure='layerwise',\n",
    "                                             prior_prec_init=1.,\n",
    "                                             sigma_noise_init=1.,\n",
    "                                             temperature=1.,\n",
    "                                             n_epochs=1,\n",
    "                                             lr=1e-3,\n",
    "                                             lr_min=None,\n",
    "                                             optimizer='adam',\n",
    "                                             scheduler='cos',\n",
    "                                             n_epochs_burnin=0,\n",
    "                                             n_hypersteps=1,\n",
    "                                             marglik_frequency=1,\n",
    "                                             lr_hyp=1e-1,\n",
    "                                             lr_hyp_min=1e-1,\n",
    "                                             laplace=KronLaplace,\n",
    "                                             backend=AsdlGGN,\n",
    "                                             early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097fcd6",
   "metadata": {},
   "source": [
    "## PAC-Bayes with Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "hello noaym\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from PBB.pbb.utils import runexp\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "BATCH_SIZE = 250\n",
    "TRAIN_EPOCHS = 50\n",
    "DELTA = 0.025\n",
    "DELTA_TEST = 0.01\n",
    "PRIOR = 'rand'\n",
    "\n",
    "SIGMAPRIOR = 0.03\n",
    "\n",
    "PMIN = 1e-5\n",
    "KL_PENALTY = 0.1\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.95\n",
    "LEARNING_RATE_PRIOR = 0.005\n",
    "MOMENTUM_PRIOR = 0.99\n",
    "\n",
    "# note the number of MC samples used in the paper is 150.000, which usually takes a several hours to compute\n",
    "MC_SAMPLES = 1000\n",
    "\n",
    "# note all of these running examples have different settings!\n",
    "\n",
    "runexp('mnist', 'fquad', PRIOR, 'fcn', SIGMAPRIOR, PMIN, LEARNING_RATE, MOMENTUM, LEARNING_RATE_PRIOR, MOMENTUM_PRIOR, delta=DELTA, delta_test=DELTA_TEST, mc_samples=MC_SAMPLES, train_epochs=TRAIN_EPOCHS, device=DEVICE, perc_train=1.0, verbose=True, dropout_prob=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
